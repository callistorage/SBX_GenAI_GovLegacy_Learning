# Lesson 1 Instructor Guide: AI Tool Fundamentals for Legacy Development

## Lesson Overview

**Target Duration**: 45-60 minutes  
**Learning Level**: Beginner (assuming no prior AI tool experience)  
**Prerequisites**: Basic programming knowledge, access to AI coding assistant

## Teaching Objectives

By the end of this lesson, students should demonstrate:
1. **Setup competency**: Proper AI tool configuration for government development
2. **Prompting effectiveness**: Context-rich prompts that yield useful results
3. **Multi-technology application**: AI assistance across COBOL, Java, and SQL
4. **Critical evaluation**: Recognition of AI tool limitations in government contexts

## Pre-Lesson Preparation

### Instructor Setup (15 minutes before class)
- [ ] Test AI tool access and functionality with provided code samples
- [ ] Prepare example prompts and expected responses  
- [ ] Review government development context materials
- [ ] Set up shared screen for demonstrations

### Student Preparation Requirements
- [ ] AI coding assistant installed and accessible (GitHub Copilot, Claude Code, etc.)
- [ ] Text editor or IDE with AI integration
- [ ] Access to lesson materials and code samples
- [ ] Basic familiarity with at least one programming language

## Detailed Teaching Guide

### Part 1: AI Tool Setup and Configuration (15 minutes)

#### Opening Discussion (3 minutes)
**Discussion prompt**: "What challenges do you face when working with unfamiliar legacy code?"

**Expected responses**:
- Understanding business logic
- Deciphering old coding patterns
- Lack of documentation
- Fear of breaking existing functionality

**Key teaching point**: Position AI tools as assistants for these exact challenges, not replacements for developer judgment.

#### Demonstration: Effective Tool Setup (7 minutes)

**Live Demo**: Show proper AI tool configuration
1. **Privacy settings review**: Emphasize government data sensitivity
2. **Context configuration**: Demonstrate how to set project-specific settings
3. **Basic functionality test**: Use the simple COBOL prompt from the lesson

**Teaching tip**: Have students follow along rather than just watch. Pause for questions.

#### Hands-On Practice (5 minutes)
Students complete Exercise 1.1 while instructor circulates to help with setup issues.

**Common student issues**:
- Tool authentication problems
- Unclear privacy settings
- Inconsistent AI responses

**Instructor responses**:
- Have backup authentication steps ready
- Clarify that AI response variation is normal
- Emphasize the importance of proper configuration documentation

### Part 2: Basic Prompting Strategies (20 minutes)

#### Core Concept Introduction (5 minutes)
**Key teaching points**:
- AI tools are only as good as the context you provide
- Government legacy systems have unique patterns AI may not recognize
- Always start with understanding before making changes

**Teaching strategy**: Use the "bad prompt vs. good prompt" comparison:

**Bad prompt**: "Fix this code"
**Good prompt**: "Analyze this COBOL code from a government benefits system and explain what business process it implements"

#### Guided Practice with Templates (10 minutes)

**Instructor approach**: 
1. **Demonstrate** each template with one code sample
2. **Guide** students through second sample together  
3. **Independent practice** on third sample

**Teaching tip**: Encourage students to modify the templates for their specific context rather than using them verbatim.

#### Student Practice and Discussion (5 minutes)
Students work on Exercise 1.2 while instructor:
- **Observes** different prompting approaches
- **Collects** examples of effective and ineffective prompts
- **Prepares** to share best practices during debrief

### Part 3: Multi-Technology Practice (15 minutes)

#### Scenario Introduction (3 minutes)
**Teaching approach**: Frame this as a realistic government scenario
- Emphasize the interconnected nature of legacy systems
- Highlight that AI tools must understand system integration
- Set expectation that this is more complex than single-file analysis

#### Collaborative Exercise (10 minutes)
**Instructor role during Exercise 1.3**:
- **Circulate** and observe student approaches
- **Ask guiding questions**: "How do these three components communicate?"
- **Encourage cross-component thinking**: "What happens if the COBOL calculation changes?"
- **Document interesting insights** for group discussion

#### Group Debrief (2 minutes)
**Discussion questions**:
- "What was more challenging: understanding individual components or their interactions?"
- "Where did your AI tool struggle with the system integration aspects?"
- "What additional context would have improved AI responses?"

### Part 4: Limitations and Best Practices (10 minutes)

#### Critical Analysis Activity (5 minutes)
**Teaching strategy**: Use Exercise 1.4 to demonstrate AI limitations through experience rather than lecture.

**Instructor guidance**:
- **Encourage experimentation**: "Try to push your AI tool beyond its comfort zone"
- **Document patterns**: Note common limitation areas across student experiences
- **Avoid criticism**: Frame limitations as "areas where human expertise remains essential"

#### Best Practices Synthesis (5 minutes)
**Interactive approach**: Build the best practices list from student experiences

**Guiding questions**:
- "Based on your experience, when would you trust AI suggestions?"
- "What government-specific knowledge did you have to provide?"
- "How would you explain these tools to a colleague?"

## Assessment Guidance

### Practical Challenge Evaluation

**Time management**: Encourage students to focus on process over perfection
- **5 minutes per task** is a guideline, not a strict limit
- **Quality over quantity**: Better to do fewer tasks well
- **Document the process**: Emphasize learning from the prompting experience

**Evaluation criteria**:
- **Prompt effectiveness**: Are students providing sufficient context?
- **Critical thinking**: Do they recognize AI limitations?
- **Government awareness**: Are they considering government-specific factors?
- **Process understanding**: Can they explain their approach?

### Reflective Analysis Discussion

**Facilitation approach**:
- **Share examples** from student work (with permission)
- **Identify patterns** across student experiences
- **Connect to real-world scenarios**: "How does this apply to your work environment?"
- **Prepare for next lesson**: "What would help you analyze legacy code more effectively?"

## Common Student Challenges and Solutions

### Challenge 1: "The AI doesn't understand COBOL very well"
**Teaching response**: 
- This is a real limitation to acknowledge
- Demonstrate how additional context improves results
- Show that understanding can be built incrementally
- Emphasize that AI assists rather than replaces COBOL knowledge

### Challenge 2: "I don't know enough about government systems to provide good context"
**Teaching response**:
- Position this as a learning opportunity, not a deficit
- Provide examples of government context that improves AI responses
- Encourage questions about government development patterns
- Connect to next lesson where we'll build this knowledge

### Challenge 3: "Different AI tools give different answers"
**Teaching response**:
- Acknowledge this is normal and expected
- Emphasize the importance of critical evaluation
- Show how to compare and synthesize different AI responses
- Use as opportunity to discuss tool-agnostic principles

### Challenge 4: "I'm not sure when to trust the AI suggestions"
**Teaching response**:
- Excellent question that shows good critical thinking
- Develop criteria based on student experience
- Emphasize incremental validation approach
- Connect to government change control requirements

## Extension Activities

### For Fast Finishers
1. **Comparative analysis**: Try the same prompts with different AI tools
2. **Prompt optimization**: Refine prompts to get better responses
3. **Additional code samples**: Analyze extra legacy code examples
4. **Integration scenarios**: Create connections between different code samples

### For Struggling Students
1. **Simplified prompts**: Start with very basic questions about code functionality
2. **Guided practice**: Work through prompting strategies step-by-step
3. **Partner work**: Pair with more confident students
4. **Focus on one technology**: Master prompting for one language before expanding

## Demo-Focused Alternative Delivery

### When to Use Demo Format
- Large groups (>20 people)
- Limited time (30 minutes or less)
- Mixed technical skill levels
- Introduction/overview session

### Demo Adaptation Strategy
1. **Interactive demonstrations**: Ask audience to suggest prompts
2. **Think-aloud process**: Verbalize decision-making during AI tool usage
3. **Comparative examples**: Show multiple approaches to same problem
4. **Q&A integration**: Pause for questions and discussion throughout
5. **Takeaway resources**: Provide templates and examples for later practice

### Demo-Specific Materials Needed
- [ ] Large screen/projector setup
- [ ] Pre-prepared AI tool environment
- [ ] Example prompts ready to type
- [ ] Backup examples in case of technical issues
- [ ] Handouts with key prompting templates

## Preparation for Lesson 2

### Student Preparation
- Review additional COBOL and legacy Java resources (optional)
- Identify legacy code from their work environment for practice (if permitted)
- Consider questions about code documentation challenges

### Instructor Preparation
- Collect examples of effective student prompts from this lesson
- Prepare more complex code analysis scenarios
- Review advanced prompting strategies for documentation generation

## Resources and References

### Government Context Resources
- [Federal Secure Coding Standards](https://www.cisa.gov/secure-coding-standards)
- [NIST Cybersecurity Framework](https://www.nist.gov/cyberframework)
- [OMB Software Development Guidelines](https://www.whitehouse.gov/omb/)

### AI Tool Documentation
- [GitHub Copilot Government Usage Guidelines](https://docs.github.com/en/copilot)
- [AI Tool Privacy and Security Considerations](https://example.com)

### Legacy Technology References
- [IBM COBOL Programming Guide](https://www.ibm.com/docs/en/cobol-zos)
- [Legacy Java Enterprise Patterns](https://docs.oracle.com/javase/8/)
- [Government Database Standards](https://www.data.gov/)

---

## Teaching Notes and Adaptations

### For Government Agency Training
- **Emphasize security considerations** throughout
- **Include agency-specific examples** where possible
- **Address procurement and approval processes** for AI tools
- **Connect to existing development standards and practices**

### For Academic Settings
- **Provide broader context** about government IT challenges
- **Include career preparation** aspects
- **Connect to public service technology** themes
- **Encourage research projects** on government modernization

### For Industry Training
- **Focus on contractor perspectives** and requirements
- **Emphasize compliance and security clearance** considerations
- **Include business development** applications
- **Address client relationship** aspects of government work

---

*Instructor Guide for Lesson 1 of 10 | Teaching time: 45-60 minutes | Prep time: 20-30 minutes*
